from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
import ast
from tools import make_file_txt, make_file_docx, make_invoice

class ChatBot:
    def __init__(self):
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
        self.llm_with_tools = llm.bind_tools([make_file_txt, make_file_docx, make_invoice])

        self.memory = ConversationBufferMemory(memory_key= "history", return_messages=True)

        prompt = ChatPromptTemplate([
            ('system', "B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o, b·∫°n c√≥ th·ªÉ t·∫°o file txt v√† docx v·ªõi n·ªôi dung ƒë∆∞·ª£c cung c·∫•p."),
            MessagesPlaceholder(variable_name= 'history'),
            ("human", '{input}')
        ])
        # K·∫øt h·ª£p prompt v·ªõi LLM v√† tools   
        self.chain = prompt | self.llm_with_tools

    def process_tool_calls(self, ai_msg: AIMessage, memory: ConversationBufferMemory) -> None:
        # Mapping t√™n function -> tool object
        tool_map = {
            "make_file_txt": make_file_txt,
            "make_file_docx": make_file_docx,
            "make_invoice": make_invoice
        }

        if isinstance(ai_msg, AIMessage) and ai_msg.tool_calls:
            for call in ai_msg.tool_calls:
                tool_name = call["name"]
                tool_args = call["args"]

                print(f"üîß Tool call: {tool_name} v·ªõi args: {tool_args}")

                # Bi·∫øn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu c·ªßa data_input n·∫øu c·∫ßn thi·∫øt
                # ƒê√¢y l√† chu·ªói d·∫°ng dict, kh√¥ng ph·∫£i dict th·∫≠t D√πng json.loads() l·∫°i b·ªã l·ªói v√¨:
                # JSON b·∫Øt bu·ªôc ph·∫£i d√πng " (double quote) ‚Üí 'ngay' l√† kh√¥ng h·ª£p l·ªá
                # D·∫´n ƒë·∫øn Pydantic b√°o l·ªói ValidationError: data_input ph·∫£i l√† dict
                # V√¨ n√≥ c√≥ th·ªÉ parse 'single-quoted dict' ‚Üí dict
                # json.loads() ch·ªâ d√πng ƒë∆∞·ª£c v·ªõi JSON chu·∫©n (double quotes, kh√¥ng comment)
 
                if isinstance(tool_args, dict) and "data_input" in tool_args and isinstance(tool_args["data_input"], str):
                    try:
                        tool_args["data_input"] = ast.literal_eval(tool_args["data_input"])
                    except Exception as e:
                        print(f"‚ùå L·ªói parse tool_args['data_input']: {e}")
                        continue
                    
                # G·ªçi tool n·∫øu h·ª£p l·ªá
                if tool_name in tool_map:
                    try:
                        tool_map[tool_name].invoke(tool_args)
                        
                        # L∆∞u l·∫°i v√†o memory sau khi g·ªçi tool ƒë√°nh d·∫•u cho llm bi·∫øt ƒë√£ th·ª±c hi·ªán tool call
                        memory.save_context(
                            {"input": "Y√™u c·∫ßu"},
                            {"output": f"ƒê√£ ch·∫°y tool {tool_name} v·ªõi args: {tool_args}, tho√°t kh·ªèi ch·∫ø ƒë·ªô tool call"}
                        )
                    except Exception as e:
                        print(f"L·ªói khi invoke tool {tool_name}: {e}")
                else:
                    print(f"Tool ch∆∞a ƒë·ªãnh nghƒ©a: {tool_name}")

    def chat(self, message: str):
        # T·∫°o input context
        inputs = {
            "input": message,
            "history": self.memory.load_memory_variables({})["history"]
        }

        response = self.chain.invoke(inputs)

        # L∆∞u v√†o memory
        self.memory.save_context(
            {"input": message},
            {"output": response.content}
        )
        print("C√¢u h·ªèi: ", message)
        
        print("Tr·∫£ l·ªùi: ", response.content)
        print("--" * 50)
        self.process_tool_calls(response, self.memory)
