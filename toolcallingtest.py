from langchain.memory import ConversationBufferMemory
from langchain.prompts import  (ChatPromptTemplate,
                                SystemMessagePromptTemplate,
                                HumanMessagePromptTemplate,
                                MessagesPlaceholder)
from langchain_core.messages import AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
import ast
from tools import make_file_txt, make_file_docx, make_invoice

system_promt = "B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o th√¥ng minh. B·∫°n c√≥ th·ªÉ t·∫°o file .txt v√† .docx v·ªõi n·ªôi dung ƒë∆∞·ª£c ng∆∞·ªùi d√πng cung c·∫•p."
class ChatBot:
    def __init__(self):
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
        self.llm_with_tools = llm.bind_tools([make_file_txt, make_file_docx, make_invoice])

        self.memory = ConversationBufferMemory(memory_key= "history", return_messages=True)

        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(
                system_promt
            ),
            MessagesPlaceholder(variable_name="history"),
            HumanMessagePromptTemplate.from_template("{input}")
        ])

        # K·∫øt h·ª£p prompt v·ªõi LLM v√† tools   
        self.chain = prompt | self.llm_with_tools

    def process_tool_calls(self, ai_msg: AIMessage, memory: ConversationBufferMemory) -> None:
        # Mapping t√™n function -> tool object
        tool_map = {
            "make_file_txt": make_file_txt,
            "make_file_docx": make_file_docx,
            "make_invoice": make_invoice
        }

        if isinstance(ai_msg, AIMessage) and ai_msg.tool_calls:
            for call in ai_msg.tool_calls:
                tool_name = call["name"]
                tool_args = call["args"]

                print(f"üîß Tool call: {tool_name} v·ªõi args: {tool_args}")

                # Bi·∫øn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu c·ªßa data_input n·∫øu c·∫ßn thi·∫øt
                # ƒê√¢y l√† chu·ªói d·∫°ng dict, kh√¥ng ph·∫£i dict th·∫≠t D√πng json.loads() l·∫°i b·ªã l·ªói v√¨:
                # JSON b·∫Øt bu·ªôc ph·∫£i d√πng " (double quote) ‚Üí 'ngay' l√† kh√¥ng h·ª£p l·ªá
                # D·∫´n ƒë·∫øn Pydantic b√°o l·ªói ValidationError: data_input ph·∫£i l√† dict
                # V√¨ n√≥ c√≥ th·ªÉ parse 'single-quoted dict' ‚Üí dict
                # json.loads() ch·ªâ d√πng ƒë∆∞·ª£c v·ªõi JSON chu·∫©n (double quotes, kh√¥ng comment)
 
                if isinstance(tool_args, dict) and "data_input" in tool_args and isinstance(tool_args["data_input"], str):
                    try:
                        tool_args["data_input"] = ast.literal_eval(tool_args["data_input"])
                    except Exception as e:
                        print(f"‚ùå L·ªói parse tool_args['data_input']: {e}")
                        continue
                    
                # G·ªçi tool n·∫øu h·ª£p l·ªá
                if tool_name in tool_map:
                    try:
                        # invoke ch·∫°y tool t∆∞∆°ng ·ª©ng v·ªõi tham s·ªë tool_args t∆∞∆°ng ·ª©ng
                        tool_map[tool_name].invoke(tool_args)
                        
                        # L∆∞u l·∫°i v√†o memory sau khi g·ªçi tool ƒë√°nh d·∫•u cho llm bi·∫øt ƒë√£ th·ª±c hi·ªán tool call
                        memory.save_context(
                            {"input": "Y√™u c·∫ßu"},
                            {"output": f"ƒê√£ ch·∫°y tool {tool_name} v·ªõi args: {tool_args}, tho√°t kh·ªèi ch·∫ø ƒë·ªô tool call"}
                        )
                    except Exception as e:
                        memory.save_context(
                            {"input": "Y√™u c·∫ßu"},
                            {"output": f"Y√™u c·∫ßu ch·∫°y {tool_name} v·ªõi args: {tool_args} ch∆∞a ƒë∆∞·ª£c th·ª±c hi·ªán do l·ªói: {e}"}
                        )
                else:
                    print(f"Tool ch∆∞a ƒë·ªãnh nghƒ©a: {tool_name}")

    def chat(self, message: str):
        # T·∫°o input context
        inputs = {
            "input": message,
            "history": self.memory.load_memory_variables({})["history"]
        }

        response = self.chain.invoke(input= inputs)

        # L∆∞u v√†o memory
        self.memory.save_context(
            inputs= {"input": message},
            outputs= {"output": response.content}
        )
        self.process_tool_calls(response, self.memory)
        return response.content 


# --- T·∫°o ƒë·ªëi t∆∞·ª£ng ChatLLM --- for testing
chat_llm = ChatBot()
while True:
    user_input = input("B·∫°n: ")
    if user_input.lower() in ["exit", "quit", "bye"]:
        print("K·∫øt th√∫c cu·ªôc tr√≤ chuy·ªán. T·∫°m bi·ªát!")
        break
    response = chat_llm.chat(user_input)
    print("AI:", response)
    # print("\n--- L·ªãch s·ª≠ tr√≤ chuy·ªán ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t ---")
    # print(chat_llm.memory.buffer)
    print("-" * 100)